import streamlit as st
import pandas as pd
import joblib
import numpy as np
import os
import plotly.express as px
import plotly.graph_objects as go
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix

# ==========================================
# 1. CONFIG & CSS
# ==========================================
st.set_page_config(page_title="NIDS Report", layout="wide")
pd.set_option("styler.render.max_elements", 5000000)
current_dir = os.path.dirname(os.path.abspath(__file__))

# COLOR SCHEME (Gi·ªØ nguy√™n theo y√™u c·∫ßu c·ªßa b·∫°n)
RED_MAIN = '#FF0000'  # Red for key metrics values
BLACK_MAIN = '#000000' # Black for text
GRAY_SUB = '#808080'   # Gray for subtext

st.markdown(f"""
    <style>
    /* Font */
    h1, h2, h3 {{ font-family: 'Arial', sans-serif; color: {BLACK_MAIN}; }}
    
    /* Buttons */
    .stButton>button {{
        width: 100%; border-radius: 4px; height: 45px; 
        font-weight: bold; text-transform: uppercase;
        background-color: white; color: black; border: 1px solid black;
    }}
    .stButton>button:hover {{ background-color: #f0f0f0; color: black; }}
    
    /* Tabs */
    .stTabs [aria-selected="true"] {{ border-bottom: 4px solid {RED_MAIN}; color: {RED_MAIN}; font-weight: bold; }}
    
    /* Metrics */
    div[data-testid="stMetricLabel"] {{
        color: {GRAY_SUB}; font-size: 16px; font-weight: 600;
    }}
    div[data-testid="stMetricValue"] {{
        color: {RED_MAIN}; font-size: 30px; font-weight: bold;
    }}
    </style>
""", unsafe_allow_html=True)

# ==========================================
# 2. LOAD SYSTEM
# ==========================================
@st.cache_resource
def load_system(ds_name):
    path = os.path.join(current_dir, 'model_storage', ds_name)
    try:
        models = joblib.load(os.path.join(path, 'all_models.pkl'))
        scaler = joblib.load(os.path.join(path, 'scaler.pkl'))
        pca = joblib.load(os.path.join(path, 'pca.pkl'))
        metadata = joblib.load(os.path.join(path, 'metadata.pkl'))
        base_perf = joblib.load(os.path.join(path, 'performance.pkl'))
        return models, scaler, pca, metadata, base_perf
    except Exception as e:
        # st.error(f"Error loading system: {e}") # C√≥ th·ªÉ b·ªè qua l·ªói n√†y khi ch·∫°y l·∫ßn ƒë·∫ßu
        return None, None, None, None, None

# SIDEBAR
st.sidebar.title("SETUP")
dataset = st.sidebar.selectbox("Dataset:", ("NSL-KDD", "CICIDS2017"))
st.sidebar.markdown("---")
mode = st.sidebar.radio("Function:", ["1. Audit", "2. Benchmark", "3. Dashboard"])

models, scaler, pca, metadata, base_perf = load_system(dataset)
if not models:
    st.error(f"Error: Kh√¥ng t√¨m th·∫•y models cho {dataset}. Vui l√≤ng ch·∫°y file `train_system.py` tr∆∞·ªõc."); st.stop()

# PREPROCESS
def preprocess(df_in):
    df = df_in.copy()
    ignore = ['class', 'Label', ' Label', 'id', 'Destination Port', 'Flow ID', 'Source IP', 'Source Port', 'Destination IP', 'Timestamp']
    df.drop([c for c in ignore if c in df.columns], axis=1, inplace=True, errors='ignore')
    
    if dataset == 'CICIDS2017':
        df.columns = df.columns.str.strip()
        df.replace([np.inf, -np.inf], np.nan, inplace=True); df.fillna(0, inplace=True)
        df = df.select_dtypes(include=[np.number])
    if dataset == 'NSL-KDD':
        df.drop(['num_outbound_cmds', 'is_host_login'], axis=1, inplace=True, errors='ignore')
        categorical_cols = [col for col in ['protocol_type', 'service', 'flag'] if col in df.columns]
        if categorical_cols:
            df = pd.get_dummies(df, columns=categorical_cols)
        
    # Reindex to match training columns
    df = df.reindex(columns=metadata['final_columns'], fill_value=0)
    
    # Scale and apply PCA
    scaled = scaler.transform(df)
    transformed = pca.transform(scaled)
    return transformed

if 'pred_done' not in st.session_state: 
    st.session_state.update({'pred_done': False, 'y_pred': None, 'df_input': None})

# ==========================================
# 3. MAIN INTERFACE
# ==========================================
st.title(f"H·ªá th·ªëng Ph√°t hi·ªán X√¢m nh·∫≠p (NIDS) - {dataset}")
st.markdown("---")

# --- MODE 1: AUDIT ---
if mode == "1. Audit":
    st.subheader("Ki·ªÉm th·ª≠ T·ª´ng M√¥ h√¨nh")
    c1, c2 = st.columns([1, 3])
    with c1:
        st.subheader("Tham s·ªë")
        model_name = st.selectbox("M√¥ h√¨nh:", list(models.keys()), index=3)
        st.markdown("---")
        f_in = st.file_uploader("1. Input Data (.csv)", type="csv")
    with c2:
        if f_in:
            df = pd.read_csv(f_in)
            st.write(f"D·ªØ li·ªáu ƒë·∫ßu v√†o: `{f_in.name}` ({len(df)} d√≤ng)")
            if st.button("TH·ª∞C HI·ªÜN PH√ÇN LO·∫†I"):
                try:
                    st.session_state.y_pred = models[model_name].predict(preprocess(df))
                    st.session_state.df_input = df
                    st.session_state.pred_done = True
                    st.success("Ph√¢n lo·∫°i ho√†n t·∫•t!")
                except Exception as e: st.error(f"L·ªói: {e}")

        if st.session_state.pred_done:
            y_p = st.session_state.y_pred
            st.subheader("K·∫øt qu·∫£ Ph√¢n lo·∫°i")
            k1, k2 = st.columns(2)
            k1.metric("B√¨nh th∆∞·ªùng (0)", np.sum(y_p == 0))
            k2.metric("T·∫•n c√¥ng (1)", np.sum(y_p == 1))
            st.markdown("---")
            
            f_tr = st.file_uploader("2. Ground Truth (.csv) ƒë·ªÉ so s√°nh", type="csv")
            if f_tr:
                df_tr = pd.read_csv(f_tr)
                if st.button("SO S√ÅNH & T√åM L·ªñI"):
                    lbl = next((c for c in ['class', 'Label', ' Label'] if c in df_tr.columns), None)
                    if lbl and len(df_tr) == len(st.session_state.df_input):
                        y_t = df_tr[lbl].apply(lambda x: 0 if str(x) in ['0', 'normal', 'BENIGN'] else 1).values
                        
                        # T√≠nh CM ƒë·ªÉ l·∫•y 4 gi√° tr·ªã
                        cm_array = confusion_matrix(y_t, y_p)
                        TN, FP, FN, TP = cm_array.ravel()
                        
                        m1, m2, m3, m4 = st.columns(4)
                        m1.metric("Accuracy", f"{accuracy_score(y_t, y_p):.2%}")
                        m2.metric("Precision", f"{precision_score(y_t, y_p, zero_division=0):.2%}")
                        m3.metric("Recall (ƒê·ªô nh·∫°y)", f"{recall_score(y_t, y_p, zero_division=0):.2%}")
                        m4.metric("F1-Score", f"{f1_score(y_t, y_p, zero_division=0):.2%}")
                        
                        st.markdown("#### Ph√¢n T√≠ch ƒê·ªô Sai L·ªách (Confusion Matrix)")
                        c_cm_m1, c_cm_m2, c_cm_m3, c_cm_m4 = st.columns(4)
                        
                        c_cm_m1.metric("TN (B√¨nh th∆∞·ªùng ƒë√∫ng)", f"{TN}", delta="G√≥i tin an to√†n", delta_color="normal")
                        c_cm_m2.metric("TP (T·∫•n c√¥ng ƒë√∫ng)", f"{TP}", delta="Ph√°t hi·ªán th√†nh c√¥ng", delta_color="inverse")
                        c_cm_m3.metric("FP (B√°o ƒë·ªông gi·∫£)", f"{FP}", delta="L·ªói c·∫£nh b√°o sai", delta_color="inverse")
                        c_cm_m4.metric("FN (B·ªè s√≥t t·∫•n c√¥ng)", f"{FN}", delta="L·ªói nguy hi·ªÉm", delta_color="inverse")
                        
                        err = np.where(y_t != y_p)[0]
                        if len(err) > 0:
                            st.error(f"Sai l·ªách: {len(err)} m·∫´u.")
                            
                            df_debug = st.session_state.df_input.copy()
                            df_debug['Actual'] = y_t; df_debug['Predicted'] = y_p
                            
                            ec1, ec2 = st.columns(2)
                            with ec1: 
                                missed = df_debug[(df_debug['Actual']==1) & (df_debug['Predicted']==0)]
                                st.write(f"**B·ªè s√≥t t·∫•n c√¥ng (False Negatives): {len(missed)}**")
                                if not missed.empty: st.dataframe(missed.head(100))
                            with ec2:
                                false_alarm = df_debug[(df_debug['Actual']==0) & (df_debug['Predicted']==1)]
                                st.write(f"**B√°o ƒë·ªông gi·∫£ (False Positives): {len(false_alarm)}**")
                                if not false_alarm.empty: st.dataframe(false_alarm.head(100))
                                
                            fig_cm = px.imshow(confusion_matrix(y_t, y_p), text_auto=True, aspect="equal", 
                                               color_continuous_scale='Greys', 
                                               x=['Normal', 'Anomaly'], y=['Normal', 'Anomaly'],
                                               labels=dict(x="Predicted", y="Actual"))
                            fig_cm.update_layout(title="Confusion Matrix", width=400, height=400)
                            fig_cm.update_coloraxes(showscale=False)
                            # FIX: Th√™m key duy nh·∫•t
                            st.plotly_chart(fig_cm, key="audit_cm_chart")
                        else: 
                            st.success("Ch√≠nh x√°c 100%.")
                    else: st.error("L·ªói file nh√£n ho·∫∑c s·ªë d√≤ng kh√¥ng kh·ªõp.")

# --- MODE 2: BENCHMARK ---
elif mode == "2. Benchmark":
    st.subheader("ƒê√°nh gi√° To√†n di·ªán (Benchmark)")
    st.write("Ch·∫°y so s√°nh 5 m√¥ h√¨nh tr√™n t·∫≠p d·ªØ li·ªáu m·ªõi.")
    
    c1, c2 = st.columns(2)
    with c1:
        f_features = st.file_uploader("T·∫£i File D·ªØ li·ªáu Test (KH√îNG C√ì NH√ÉN)", key='bench_x', type="csv")
    with c2:
        f_labels = st.file_uploader("T·∫£i File Nh√£n G·ªëc (GROUND TRUTH)", key='bench_y', type="csv")
    
    if f_features and f_labels:
        df_x = pd.read_csv(f_features)
        df_y = pd.read_csv(f_labels)
        
        lbl = next((c for c in ['class', 'Label', ' Label'] if c in df_y.columns), None)
        
        if lbl and len(df_x) == len(df_y):
            st.success(f"Hai file kh·ªõp nhau ({len(df_x)} d√≤ng). S·∫µn s√†ng Benchmark.")
            if st.button("B·∫ÆT ƒê·∫¶U ƒê√ÅNH GI√Å T·∫§T C·∫¢ M√î H√åNH"):
                
                X = preprocess(df_x)
                y_t = df_y[lbl].apply(lambda x: 0 if str(x) in ['0', 'normal', 'BENIGN'] else 1).values
                
                res = {}
                prog = st.progress(0, text="ƒêang d·ª± ƒëo√°n v√† t√≠nh to√°n hi·ªáu nƒÉng...")
                
                for i, (name, m) in enumerate(models.items()):
                    yp = m.predict(X)
                    cm_array = confusion_matrix(y_t, yp)
                    TN, FP, FN, TP = cm_array.ravel() 
                    
                    res[name] = {
                        'Accuracy': accuracy_score(y_t, yp), 'F1-Score': f1_score(y_t, yp, zero_division=0),
                        'Recall': recall_score(y_t, yp, zero_division=0), 'Precision': precision_score(y_t, yp, zero_division=0),
                        'TN': TN, 'FP': FP, 'FN': FN, 'TP': TP,
                        'CM': cm_array
                    }
                    prog.progress((i+1)/5, text=f"ƒêang ƒë√°nh gi√° {name}...")
                
                prog.empty()
                st.markdown("---")
                st.subheader("2. K·∫øt Qu·∫£ Benchmark")
                
                # 1. B·∫¢NG X·∫æP H·∫†NG
                st.markdown("### B·∫£ng X·∫øp H·∫°ng")
                df_res = pd.DataFrame(res).T.reset_index().rename(columns={'index':'Model'})
                
                df_display = df_res.drop(columns=['CM'], errors='ignore') 
                
                metric_cols_all = ['Accuracy', 'F1-Score', 'Recall', 'Precision', 'TN', 'FP', 'FN', 'TP']
                
                df_styled = df_display.style.highlight_max(axis=0, color='#FFC0CB', subset=metric_cols_all)
                # FIX WARNING: D√πng width thay v√¨ use_container_width (n·∫øu phi√™n b·∫£n c≈©, gi·ªØ nguy√™n tham s·ªë c≈© c≈©ng ƒë∆∞·ª£c, ·ªü ƒë√¢y t√¥i d√πng use_container_width cho b·∫£n m·ªõi)
                st.dataframe(df_styled, use_container_width=True)
                
                # Bi·ªÉu ƒë·ªì c·ªôt so s√°nh
                df_chart = df_display.drop(columns=['TN', 'FP', 'FN', 'TP']).melt(id_vars='Model', value_vars=['Accuracy', 'F1-Score', 'Recall'])
                fig = px.bar(df_chart, x='Model', y='value', color='variable', barmode='group', 
                             text_auto='.2%', height=400, 
                             color_discrete_sequence=[RED_MAIN, BLACK_MAIN, GRAY_SUB], 
                             title="Comparison of Key Metrics")
                # FIX: Th√™m key duy nh·∫•t
                st.plotly_chart(fig, key="bench_bar_chart")
                
                # 2. So s√°nh Train vs Test 
                st.markdown("### So s√°nh v·ªõi l√∫c hu·∫•n luy·ªán (Train vs Test)")
                df_train = pd.DataFrame(base_perf).T.reset_index().rename(columns={'index': 'Model'})
                df_train.columns = [c + ' (Train)' if c != 'Model' else c for c in df_train.columns]
                
                df_bench = df_display.drop(columns=['TN', 'FP', 'FN', 'TP'], errors='ignore').copy() 
                df_bench.columns = [c + ' (Benchmark)' if c != 'Model' else c for c in df_bench.columns]
                df_merged = pd.merge(df_train, df_bench, on='Model', how='inner')
                
                for m in ['Accuracy', 'F1-Score']:
                    df_merged[m + ' Delta'] = df_merged[m + ' (Benchmark)'] - df_merged[m + ' (Train)']
                
                def color_delta(val):
                    if isinstance(val, (int, float)):
                        return f'color: {RED_MAIN}' if val < -0.05 else f'color: {BLACK_MAIN}'
                    return 'color: black'
                
                try:
                    st.dataframe(df_merged.style.map(color_delta, subset=[c for c in df_merged.columns if 'Delta' in c]), use_container_width=True)
                except Exception:
                    st.dataframe(df_merged, use_container_width=True)

                # 3. CHI TI·∫æT T·ª™NG M√î H√åNH
                st.markdown("### Chi Ti·∫øt T·ª´ng M√¥ H√¨nh (Ma tr·∫≠n nh·∫ßm l·∫´n)")
                tabs = st.tabs(list(models.keys()))
                for i, (k, v) in enumerate(res.items()):
                    with tabs[i]:
                        st.markdown("##### Ph√¢n T√≠ch Ph√°t Hi·ªán")
                        col_cm_m1, col_cm_m2, col_cm_m3, col_cm_m4 = st.columns(4)
                        
                        col_cm_m1.metric("TN (B√¨nh th∆∞·ªùng ƒë√∫ng)", f"{v['TN']}", delta="G√≥i tin an to√†n", delta_color="normal")
                        col_cm_m2.metric("TP (T·∫•n c√¥ng ƒë√∫ng)", f"{v['TP']}", delta="Ph√°t hi·ªán th√†nh c√¥ng", delta_color="inverse")
                        col_cm_m3.metric("FP (B√°o ƒë·ªông gi·∫£)", f"{v['FP']}", delta="L·ªói c·∫£nh b√°o sai", delta_color="inverse")
                        col_cm_m4.metric("FN (B·ªè s√≥t t·∫•n c√¥ng)", f"{v['FN']}", delta="L·ªói nguy hi·ªÉm", delta_color="inverse")
                        
                        st.markdown("##### Th·ªëng k√™ Metric v√† Bi·ªÉu ƒë·ªì")
                        c1, c2 = st.columns([1, 2])
                        c1.metric("Accuracy", f"{v['Accuracy']:.2%}")
                        c1.metric("F1-Score", f"{v['F1-Score']:.2%}")
                        c1.metric("Recall", f"{v['Recall']:.2%}")
                        
                        fig_cm = px.imshow(v['CM'], text_auto=True, aspect="equal", color_continuous_scale='Greys',
                                           x=['Normal', 'Anomaly'], y=['Normal', 'Anomaly'])
                        fig_cm.update_layout(height=350, width=350, margin=dict(l=0,r=0,t=0,b=0))
                        fig_cm.update_coloraxes(showscale=False)
                        # FIX: Th√™m key duy nh·∫•t cho m·ªói bi·ªÉu ƒë·ªì trong v√≤ng l·∫∑p (QUAN TR·ªåNG ƒê·ªÇ FIX L·ªñI CRASH)
                        c2.plotly_chart(fig_cm, key=f"bench_cm_chart_{i}")
        else:
            st.error("‚ö†Ô∏è L·ªói: ƒê·∫£m b·∫£o file Ground Truth ch·ª©a c·ªôt nh√£n (class/Label) v√† s·ªë d√≤ng kh·ªõp v·ªõi file Features.")


# --- MODE 3: DASHBOARD (C·∫¨P NH·∫¨T CHI TI·∫æT) ---
elif mode == "3. Dashboard":
    st.subheader(f"Dashboard Hi·ªáu su·∫•t Hu·∫•n luy·ªán ({dataset})")
    
    # S·∫Øp x·∫øp theo F1-Score gi·∫£m d·∫ßn
    df_base = pd.DataFrame([{'Model': m, **p} for m, p in base_perf.items()]).sort_values('F1-Score', ascending=False)
    best_model_name = df_base.iloc[0]['Model']
    best_model_data = df_base.iloc[0]
    metric_cols_dash = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    
    st.markdown(f"#### M√¥ h√¨nh T·ªët nh·∫•t: **{best_model_name}**")
    
    m1, m2, m3, m4 = st.columns(4)
    m1.metric("Accuracy", f"{best_model_data['Accuracy']:.2%}")
    m2.metric("Precision", f"{best_model_data['Precision']:.2%}")
    m3.metric("Recall (ƒê·ªô nh·∫°y)", f"{best_model_data['Recall']:.2%}")
    m4.metric("F1-Score", f"{best_model_data['F1-Score']:.2%}")
    
    st.markdown("---")

    # --- DANH S√ÅCH X·∫æP H·∫†NG CHI TI·∫æT (TEXT THU·∫¶N T√öY - KH√îNG ICON) ---
    st.markdown("#### B·∫£ng X·∫øp H·∫°ng Chi Ti·∫øt (Theo F1-Score)")
    st.info("X·∫øp h·∫°ng d·ª±a tr√™n ch·ªâ s·ªë F1-Score (ƒê·ªô c√¢n b·∫±ng gi·ªØa Precision v√† Recall).")

    for i in range(len(df_base)):
        row = df_base.iloc[i]
        rank = i + 1
        model_name = row['Model']
        stats = f"F1-Score: **{row['F1-Score']:.4f}** | Accuracy: {row['Accuracy']:.4f} | Recall: {row['Recall']:.4f}"
        
        # Hi·ªÉn th·ªã d·∫°ng text ƒë∆°n gi·∫£n "Top 1:", "Top 2:"...
        st.markdown(f"**Top {rank}: {model_name}**")
        st.markdown(f"&nbsp;&nbsp;&nbsp;&nbsp;üëâ {stats}") 
        st.write("") 
    
    st.markdown("---")
    # ----------------------------------------------------

    st.markdown("#### Bi·ªÉu ƒë·ªì So s√°nh T·ªïng quan")
    
    df_long = df_base.melt(id_vars='Model', var_name='Metric', value_name='Value')
    # FIX: Th√™m key duy nh·∫•t
    st.plotly_chart(px.bar(df_long, x='Model', y='Value', color='Metric', barmode='group', height=400, 
                           color_discrete_sequence=[RED_MAIN, BLACK_MAIN, GRAY_SUB, '#CCCCCC']), key="dash_bar_chart")
    
    c1, c2 = st.columns(2)
    with c1:
        fig_r = go.Figure()
        metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
        for i, m in enumerate(df_base['Model'].head(3)):
            v = df_base[df_base['Model']==m][metrics].values.flatten().tolist()
            color = RED_MAIN if i == 0 else (BLACK_MAIN if i == 1 else GRAY_SUB)
            dash = 'solid' if i == 0 else ('dash' if i == 1 else 'dot')
            
            fig_r.add_trace(go.Scatterpolar(r=v+[v[0]], theta=metrics+[metrics[0]], name=m, 
                                            line=dict(color=color, width=2, dash=dash), fill='toself', opacity=0.1))
        fig_r.update_layout(
            polar=dict(
                radialaxis=dict(visible=True, range=[0.5, 1], gridcolor=GRAY_SUB, linecolor=BLACK_MAIN, tickfont=dict(color=BLACK_MAIN)),
                angularaxis=dict(tickfont=dict(color=BLACK_MAIN))
            ), 
            height=400, title="Radar Chart (Top 3)", font=dict(color=BLACK_MAIN)
        )
        # FIX: Th√™m key duy nh·∫•t
        st.plotly_chart(fig_r, key="dash_radar_chart")
        
    with c2:
        fig_l = px.line(df_long, x='Model', y='Value', color='Metric', markers=True, 
                        color_discrete_sequence=[RED_MAIN, BLACK_MAIN, GRAY_SUB, '#CCCCCC'], title="Trend")
        fig_l.update_layout(height=400)
        # FIX: Th√™m key duy nh·∫•t
        st.plotly_chart(fig_l, key="dash_line_chart")

    st.markdown("---")
    st.markdown("### Performance Table")
    df_styled_dash = df_base.style.highlight_max(axis=0, color=RED_MAIN, subset=metric_cols_dash)
    st.dataframe(df_styled_dash, use_container_width=True)
    
    st.markdown("---")
    st.markdown("#### C∆° Ch·∫ø Ho·∫°t ƒê·ªông C·ªßa C√°c Thu·∫≠t To√°n Ph√¢n Lo·∫°i")
    
    tabs = st.tabs(list(models.keys()))
    
    # N·ªôi dung m√¥ t·∫£ d·ª±a tr√™n logic training
    model_visuals = {
        "Random Forest": {
            "title": "R·ª´ng Ng·∫´u Nhi√™n (Random Forest)",
            "concept": "T·ªï h·ª£p nhi·ªÅu c√¢y quy·∫øt ƒë·ªãnh ƒë·ªôc l·∫≠p, k·∫øt qu·∫£ l√† **phi·∫øu b·∫ßu ƒëa s·ªë**. Gi√∫p gi·∫£m thi·ªÉu l·ªói Overfitting v√† tƒÉng t√≠nh ·ªïn ƒë·ªãnh c·ªßa m√¥ h√¨nh. ",
        },
        "k-NN": {
            "title": "k-H√†ng X√≥m G·∫ßn Nh·∫•t (k-NN)",
            "concept": "Ph√¢n lo·∫°i d·ª±a tr√™n **kho·∫£ng c√°ch**. ƒêi·ªÉm d·ªØ li·ªáu m·ªõi ƒë∆∞·ª£c g√°n nh√£n theo l·ªõp chi·∫øm ƒëa s·ªë c·ªßa **k** ƒëi·ªÉm g·∫ßn nh·∫•t. Ph√π h·ª£p cho d·ªØ li·ªáu c√≥ ranh gi·ªõi quy·∫øt ƒë·ªãnh ph·ª©c t·∫°p. ",
        },
        "SVM": {
            "title": "M√°y Vector H·ªó Tr·ª£ (SVM)",
            "concept": "T√¨m **Si√™u m·∫∑t ph·∫≥ng (Hyperplane)** v·ªõi **Margin** l·ªõn nh·∫•t ƒë·ªÉ ph√¢n t√°ch hai l·ªõp d·ªØ li·ªáu. Ch·ªâ c√°c ƒëi·ªÉm g·∫ßn ranh gi·ªõi (Support Vectors) m·ªõi ·∫£nh h∆∞·ªüng ƒë·∫øn vi·ªác ph√¢n lo·∫°i. ",
        },
        "Decision Tree": {
            "title": "C√¢y Quy·∫øt ƒê·ªãnh (Decision Tree)",
            "concept": "C·∫•u tr√∫c d·∫°ng c√¢y ph√¢n nh√°nh (Flowchart) s·ª≠ d·ª•ng **quy t·∫Øc IF-THEN** tu·∫ßn t·ª± ƒë·ªÉ ƒë∆∞a ra quy·∫øt ƒë·ªãnh, d·ªÖ gi·∫£i th√≠ch v√† tr·ª±c quan nh·∫•t. ",
        },
        "Logistic Regression": {
            "title": "H·ªìi Quy Logistic (Logistic Regression)",
            "concept": "L√† m·ªôt m√¥ h√¨nh tuy·∫øn t√≠nh, s·ª≠ d·ª•ng h√†m **Sigmoid** ƒë·ªÉ ∆∞·ªõc t√≠nh x√°c su·∫•t. ƒê∆∞·ªùng ph√¢n chia quy·∫øt ƒë·ªãnh (Decision Boundary) l√† **tuy·∫øn t√≠nh (m·ªôt ƒë∆∞·ªùng th·∫≥ng)**. ",
        }
    }
    
    for i, model_name in enumerate(models.keys()):
        with tabs[i]:
            visual = model_visuals.get(model_name)
            if visual:
                st.markdown(f"#### {visual['title']}")
                st.write(visual['concept'])
            else:
                st.write(f"M√¥ t·∫£ cho {model_name} ƒëang ƒë∆∞·ª£c c·∫≠p nh·∫≠t.")
import pandas as pd
import numpy as np
import os
import joblib
import glob
import shutil
import logging
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# ==========================================
# COMMON FUNCTION: LOAD DATA
# ==========================================
def load_data(dataset_type, current_dir, sample_frac=0.05):
    """
    Load and preprocess data for the specified dataset type.

    Args:
        dataset_type (str): 'NSL-KDD' or 'CICIDS2017'.
        current_dir (str): Current working directory.
        sample_frac (float): Fraction to sample for CICIDS2017 to avoid RAM issues (default: 0.05).

    Returns:
        tuple: (X, y) or None if data not found.
    """
    logging.info(f"Loading data for: {dataset_type}...")

    if dataset_type == 'NSL-KDD':
        path = os.path.join(current_dir, 'data', 'NSL-KDD', 'NSL_Binary.csv')
        if not os.path.exists(path):
            logging.warning(f"Data not found for {dataset_type}. Skipping.")
            return None
        try:
            df = pd.read_csv(path)
            df.drop(['num_outbound_cmds', 'is_host_login'], axis=1, inplace=True, errors='ignore')
            df['class'] = df['class'].apply(lambda x: 1 if x == 'anomaly' else 0)
            X = df.drop('class', axis=1)
            X = pd.get_dummies(X, columns=['protocol_type', 'service', 'flag'])
            y = df['class']
            return X, y
        except Exception as e:
            logging.error(f"Error loading NSL-KDD: {e}")
            return None

    elif dataset_type == 'CICIDS2017':
        folder_path = os.path.join(current_dir, 'data', 'CICIDS2017')
        all_files = glob.glob(os.path.join(folder_path, "*.csv"))
        if not all_files:
            logging.warning(f"No CSV files found for {dataset_type}. Skipping.")
            return None

        df_list = []
        for f in all_files:
            try:
                df = pd.read_csv(f).sample(frac=sample_frac, random_state=42)
                logging.info(f"Loaded and sampled: {os.path.basename(f)}")
                df_list.append(df)
            except Exception as e:
                logging.warning(f"Error loading {os.path.basename(f)}: {e}")

        if not df_list:
            logging.warning(f"No valid data loaded for {dataset_type}.")
            return None

        df = pd.concat(df_list, ignore_index=True)
        # Preprocess CICIDS
        df.columns = df.columns.str.strip()
        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        df.dropna(inplace=True)

        if 'Label' not in df.columns:
            logging.warning("'Label' column not found in CICIDS2017 data. Skipping.")
            return None

        df['class'] = df['Label'].apply(lambda x: 0 if x == 'BENIGN' else 1)
        df.drop('Label', axis=1, inplace=True)
        # Select only numeric columns
        X = df.drop('class', axis=1).select_dtypes(include=[np.number])
        y = df['class']
        return X, y

# ==========================================
# COMMON FUNCTION: TRAIN & SAVE
# ==========================================
def train_and_save(dataset_type, sample_frac=0.05):
    """
    Train models on the dataset and save artifacts.

    Args:
        dataset_type (str): 'NSL-KDD' or 'CICIDS2017'.
        sample_frac (float): Sampling fraction for large datasets.
    """
    current_dir = os.path.dirname(os.path.abspath(__file__))

    # 1. Load Data
    data = load_data(dataset_type, current_dir, sample_frac)
    if data is None:
        logging.warning(f"No data available for {dataset_type}. Skipping training.")
        return

    X, y = data
    logging.info(f"Data for {dataset_type} ready: {X.shape}")

    # 2. Split & Preprocess
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Drop highly correlated features
    corr_matrix = X_train.corr().abs()
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]
    X_train.drop(to_drop, axis=1, inplace=True)
    X_test.drop(to_drop, axis=1, inplace=True)

    # Scale features
    scaler = MinMaxScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # 3. Training Loop
    models = {
        "Logistic Regression": LogisticRegression(max_iter=1000),
        "k-NN": KNeighborsClassifier(n_neighbors=5),
        "SVM": SVC(kernel='rbf', probability=True),
        "Decision Tree": DecisionTreeClassifier(criterion='gini', random_state=42),
        "Random Forest": RandomForestClassifier(n_estimators=50, random_state=42)
    }

    trained_models = {}
    model_performance = {}

    logging.info(f"Starting training for 5 models on {dataset_type}...")
    for name, model in models.items():
        logging.info(f"Training {name}...")
        try:
            model.fit(X_train_scaled, y_train)
            y_pred = model.predict(X_test_scaled)
            trained_models[name] = model
            model_performance[name] = {
                'Accuracy': accuracy_score(y_test, y_pred),
                'Precision': precision_score(y_test, y_pred, pos_label=1, zero_division=0),
                'Recall': recall_score(y_test, y_pred, pos_label=1, zero_division=0),
                'F1-Score': f1_score(y_test, y_pred, pos_label=1, zero_division=0)
            }
        except Exception as e:
            logging.error(f"Error training {name}: {e}")

    # 4. Save to Specific Folder
    save_path = os.path.join(current_dir, 'model_storage', dataset_type)
    if os.path.exists(save_path):
        shutil.rmtree(save_path)  # Remove old folder
    os.makedirs(save_path)

    metadata = {
        'dataset_type': dataset_type,
        'final_columns': list(X_train.columns),
        'dropped_cols': to_drop
    }

    joblib.dump(trained_models, os.path.join(save_path, 'all_models.pkl'))
    joblib.dump(scaler, os.path.join(save_path, 'scaler.pkl'))
    joblib.dump(metadata, os.path.join(save_path, 'metadata.pkl'))
    joblib.dump(model_performance, os.path.join(save_path, 'performance.pkl'))

    logging.info(f"Saved models for {dataset_type} to: model_storage/{dataset_type}")

# ==========================================
# MAIN PROGRAM
# ==========================================
if __name__ == "__main__":
    # Train both datasets sequentially
    train_and_save('NSL-KDD')
    train_and_save('CICIDS2017')
    logging.info("Completed all training processes!")
import streamlit as st
import pandas as pd
import joblib
import numpy as np
import os
import plotly.express as px
import plotly.graph_objects as go
from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix

# ==========================================
# 1. CONFIG & CSS
# ==========================================
st.set_page_config(page_title="NIDS Report", layout="wide")
pd.set_option("styler.render.max_elements", 5000000)
current_dir = os.path.dirname(os.path.abspath(__file__))

# COLOR SCHEME: White background (default), black text, red highlights
RED_MAIN = '#FF0000'  # Red for key metrics values
BLACK_MAIN = '#000000' # Black for text
GRAY_SUB = '#808080'   # Gray for subtext

st.markdown(f"""
    <style>
    /* Font */
    h1, h2, h3 {{ font-family: 'Arial', sans-serif; color: {BLACK_MAIN}; }}
    
    /* Buttons: Black on white */
    .stButton>button {{
        width: 100%; border-radius: 4px; height: 45px; 
        font-weight: bold; text-transform: uppercase;
        background-color: white; color: black; border: 1px solid black;
    }}
    .stButton>button:hover {{ background-color: #f0f0f0; color: black; }}
    
    /* Tabs: Red underline */
    .stTabs [aria-selected="true"] {{ border-bottom: 4px solid {RED_MAIN}; color: {RED_MAIN}; font-weight: bold; }}
    
    /* Metrics: Gray label, red value */
    div[data-testid="stMetricLabel"] {{
        color: {GRAY_SUB}; font-size: 16px; font-weight: 600;
    }}
    div[data-testid="stMetricValue"] {{
        color: {RED_MAIN}; font-size: 30px; font-weight: bold;
    }}
    </style>
""", unsafe_allow_html=True)

# ==========================================
# 2. LOAD SYSTEM
# ==========================================
@st.cache_resource
def load_system(ds_name):
    path = os.path.join(current_dir, 'model_storage', ds_name)
    try:
        models = joblib.load(os.path.join(path, 'all_models.pkl'))
        scaler = joblib.load(os.path.join(path, 'scaler.pkl'))
        pca = joblib.load(os.path.join(path, 'pca.pkl'))
        metadata = joblib.load(os.path.join(path, 'metadata.pkl'))
        base_perf = joblib.load(os.path.join(path, 'performance.pkl'))
        return models, scaler, pca, metadata, base_perf
    except Exception as e:
        st.error(f"Error loading system: {e}")
        return None, None, None, None, None

# SIDEBAR
st.sidebar.title("SETUP")
dataset = st.sidebar.selectbox("Dataset:", ("NSL-KDD", "CICIDS2017"))
st.sidebar.markdown("---")
mode = st.sidebar.radio("Function:", ["1. Audit", "2. Benchmark", "3. Dashboard"])

models, scaler, pca, metadata, base_perf = load_system(dataset)
if not models:
    st.error(f"Error: No models for {dataset}. Run train file first."); st.stop()

# PREPROCESS
def preprocess(df_in):
    df = df_in.copy()
    ignore = ['class', 'Label', ' Label', 'id', 'Destination Port', 'Flow ID', 'Source IP', 'Source Port', 'Destination IP', 'Timestamp']
    df.drop([c for c in ignore if c in df.columns], axis=1, inplace=True, errors='ignore')
    
    if dataset == 'CICIDS2017':
        df.columns = df.columns.str.strip()
        df.replace([np.inf, -np.inf], np.nan, inplace=True); df.fillna(0, inplace=True)
        df = df.select_dtypes(include=[np.number])
    if dataset == 'NSL-KDD':
        df.drop(['num_outbound_cmds', 'is_host_login'], axis=1, inplace=True, errors='ignore')
        categorical_cols = [col for col in ['protocol_type', 'service', 'flag'] if col in df.columns]
        if categorical_cols:
            df = pd.get_dummies(df, columns=categorical_cols)
        
    # Reindex to match training columns
    df = df.reindex(columns=metadata['final_columns'], fill_value=0)
    
    # Scale and apply PCA
    scaled = scaler.transform(df)
    transformed = pca.transform(scaled)
    return transformed

if 'pred_done' not in st.session_state: 
    st.session_state.update({'pred_done': False, 'y_pred': None, 'df_input': None})

# ==========================================
# 3. MAIN INTERFACE
# ==========================================
st.title(f"NIDS System - {dataset}")
st.markdown("---")

# --- MODE 1: AUDIT ---
if mode == "1. Audit":
    c1, c2 = st.columns([1, 3])
    with c1:
        st.subheader("Parameters")
        model_name = st.selectbox("Model:", list(models.keys()), index=3)
        st.markdown("---")
        f_in = st.file_uploader("1. Input Data (.csv)", type="csv")
    with c2:
        if f_in:
            df = pd.read_csv(f_in)
            st.write(f"Input: `{f_in.name}` ({len(df)} rows)")
            if st.button("PERFORM CLASSIFICATION"):
                try:
                    st.session_state.y_pred = models[model_name].predict(preprocess(df))
                    st.session_state.df_input = df
                    st.session_state.pred_done = True
                except Exception as e: st.error(f"Error: {e}")

        if st.session_state.pred_done:
            y_p = st.session_state.y_pred
            st.subheader("Classification Results")
            k1, k2 = st.columns(2)
            k1.metric("Normal (0)", np.sum(y_p == 0))
            k2.metric("Attack (1)", np.sum(y_p == 1))
            st.markdown("---")
            
            f_tr = st.file_uploader("2. Ground Truth (.csv) for comparison", type="csv")
            if f_tr:
                df_tr = pd.read_csv(f_tr)
                if st.button("COMPARE & FIND ERRORS"):
                    lbl = next((c for c in ['class', 'Label', ' Label'] if c in df_tr.columns), None)
                    if lbl and len(df_tr) == len(st.session_state.df_input):
                        y_t = df_tr[lbl].apply(lambda x: 0 if str(x) in ['0', 'normal', 'BENIGN'] else 1).values
                        
                        m1, m2, m3, m4 = st.columns(4)
                        m1.metric("Accuracy", f"{accuracy_score(y_t, y_p):.2%}")
                        m2.metric("Precision", f"{precision_score(y_t, y_p, zero_division=0):.2%}")
                        m3.metric("Recall", f"{recall_score(y_t, y_p, zero_division=0):.2%}")
                        m4.metric("F1-Score", f"{f1_score(y_t, y_p, zero_division=0):.2%}")
                        
                        err = np.where(y_t != y_p)[0]
                        if len(err) > 0:
                            st.error(f"Mismatches: {len(err)} samples.")
                            
                            df_debug = st.session_state.df_input.copy()
                            df_debug['Actual'] = y_t; df_debug['Predicted'] = y_p
                            
                            missed = df_debug[(df_debug['Actual']==1) & (df_debug['Predicted']==0)]
                            false_alarm = df_debug[(df_debug['Actual']==0) & (df_debug['Predicted']==1)]
                            
                            ec1, ec2 = st.columns(2)
                            with ec1: 
                                st.write(f"**Missed (False Neg): {len(missed)}**")
                                if not missed.empty: st.dataframe(missed.head(100))
                            with ec2:
                                st.write(f"**False Alarm (False Pos): {len(false_alarm)}**")
                                if not false_alarm.empty: st.dataframe(false_alarm.head(100))
                                
                            fig_cm = px.imshow(confusion_matrix(y_t, y_p), text_auto=True, aspect="equal", 
                                               color_continuous_scale='Greys', 
                                               x=['Normal', 'Anomaly'], y=['Normal', 'Anomaly'],
                                               labels=dict(x="Predicted", y="Actual"))
                            fig_cm.update_layout(title="Confusion Matrix", width=400, height=400)
                            fig_cm.update_coloraxes(showscale=False)
                            st.plotly_chart(fig_cm)
                        else: 
                            st.success("100% accurate.")
                    else: st.error("Label file error.")

# --- MODE 2: BENCHMARK ---
elif mode == "2. Benchmark":
    st.subheader("Performance Evaluation")
    f_up = st.file_uploader("Upload labeled dataset", type="csv")
    if f_up:
        df = pd.read_csv(f_up)
        lbl = next((c for c in ['class', 'Label', ' Label'] if c in df.columns), None)
        
        if lbl and st.button("START COMPARISON"):
            y_t = df[lbl].apply(lambda x: 0 if str(x) in ['0', 'normal', 'BENIGN'] else 1).values
            X = preprocess(df)
            res = {}
            
            prog = st.progress(0)
            for i, (name, m) in enumerate(models.items()):
                yp = m.predict(X)
                res[name] = {
                    'Accuracy': accuracy_score(y_t, yp), 'F1-Score': f1_score(y_t, yp, zero_division=0),
                    'Recall': recall_score(y_t, yp, zero_division=0), 'Precision': precision_score(y_t, yp, zero_division=0),
                    'CM': confusion_matrix(y_t, yp)
                }
                prog.progress((i+1)/5)
            
            st.markdown("### Ranking Table")
            df_res = pd.DataFrame(res).T.reset_index().rename(columns={'index':'Model'})
            st.dataframe(df_res[['Model', 'Accuracy', 'F1-Score', 'Recall', 'Precision']].style.highlight_max(axis=0, color='#FF0000'))

            fig = px.bar(df_res.melt(id_vars='Model', value_vars=['Accuracy', 'F1-Score', 'Recall']), 
                         x='Model', y='value', color='variable', barmode='group', 
                         text_auto='.2%', height=400, 
                         color_discrete_sequence=['#000000', '#808080', '#CCCCCC'],
                         title="Comparison of Key Metrics")
            st.plotly_chart(fig)
            
            # Added: Train vs Benchmark Comparison Table
            st.markdown("### Train vs Benchmark Comparison")
            df_train = pd.DataFrame(base_perf).T.reset_index().rename(columns={'index': 'Model'})
            df_train.columns = [col + ' (Train)' if col != 'Model' else col for col in df_train.columns]
            df_bench = df_res.copy()
            df_bench.columns = [col + ' (Benchmark)' if col != 'Model' else col for col in df_bench.columns]
            df_merged = pd.merge(df_train, df_bench, on='Model', how='inner')
            
            # Calculate deltas
            for metric in ['Accuracy', 'F1-Score', 'Recall', 'Precision']:
                df_merged[metric + ' Delta'] = df_merged[metric + ' (Benchmark)'] - df_merged[metric + ' (Train)']
            
            # Style: Highlight negative deltas (potential overfitting)
            def highlight_negative(val):
                color = 'red' if val < 0 else 'black'
                return f'color: {color}'
            
            st.dataframe(df_merged.style.applymap(highlight_negative, subset=[col for col in df_merged.columns if 'Delta' in col]))
            
            st.markdown("### Details")
            tabs = st.tabs(list(models.keys()))
            for i, (k, v) in enumerate(res.items()):
                with tabs[i]:
                    c1, c2 = st.columns([1, 2])
                    c1.metric("Accuracy", f"{v['Accuracy']:.2%}")
                    c1.metric("F1-Score", f"{v['F1-Score']:.2%}")
                    c1.metric("Recall", f"{v['Recall']:.2%}")
                    
                    fig_cm = px.imshow(v['CM'], text_auto=True, aspect="equal", color_continuous_scale='Greys',
                                       x=['Normal', 'Anomaly'], y=['Normal', 'Anomaly'])
                    fig_cm.update_layout(height=350, width=350, margin=dict(l=0,r=0,t=0,b=0))
                    fig_cm.update_coloraxes(showscale=False)
                    c2.plotly_chart(fig_cm)
        elif not lbl: st.warning("Missing label column.")

# --- MODE 3: DASHBOARD ---
elif mode == "3. Dashboard":
    st.subheader("Training Performance")
    df_base = pd.DataFrame([{'Model': m, **p} for m, p in base_perf.items()]).sort_values('F1-Score', ascending=False)
    
    st.markdown("### Performance Table")
    st.dataframe(df_base.style.highlight_max(axis=0, color='#FF0000'))
    
    df_long = df_base.melt(id_vars='Model', var_name='Metric', value_name='Value')
    st.plotly_chart(px.bar(df_long, x='Model', y='Value', color='Metric', barmode='group', height=400, 
                           color_discrete_sequence=['#000000', '#444444', '#808080', '#CCCCCC']))
    
    c1, c2 = st.columns(2)
    with c1:
        fig_r = go.Figure()
        metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
        for i, m in enumerate(df_base['Model'].head(3)):
            v = df_base[df_base['Model']==m][metrics].values.flatten().tolist()
            color = '#FF0000' if i == 0 else ('#000000' if i == 1 else '#808080')
            dash = 'solid' if i == 0 else ('dash' if i == 1 else 'dot')
            
            fig_r.add_trace(go.Scatterpolar(r=v+[v[0]], theta=metrics+[metrics[0]], name=m, 
                                            line=dict(color=color, width=2, dash=dash), fill='toself', opacity=0.1))
        fig_r.update_layout(polar=dict(radialaxis=dict(visible=True, range=[0.5, 1])), height=400, title="Radar Chart (Top 3)")
        st.plotly_chart(fig_r)
        
    with c2:
        fig_l = px.line(df_long, x='Model', y='Value', color='Metric', markers=True, 
                        color_discrete_sequence=['#000000', '#444444', '#808080', '#CCCCCC'], title="Trend")
        fig_l.update_layout(height=400)
        st.plotly_chart(fig_l)